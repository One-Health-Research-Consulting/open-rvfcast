---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# An open-source framework for Rift Valley Fever forecasting

<!-- badges: start -->
[![Project Status: WIP â€“ Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](https://www.repostatus.org/badges/latest/wip.svg)](https://www.repostatus.org/#wip)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![License (for code): MIT](https://img.shields.io/badge/License (for code)-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![License: CC0-1.0](https://img.shields.io/badge/License (for data)-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
[![License: CC-BY-4.0](https://img.shields.io/badge/License (for text)-CC_BY_4.0-blue.svg)](http://creativecommons.org/publicdomain/zero/1.0/)
<!-- badges: end -->

# Overview of OpenRVFcast
The goal of EcoHealth Alliance's ongoing OpenRVFcast project the development of a generalizable, open-source modeling framework for predicting Rift Valley Fever (RVF) outbreaks in Africa, funded by the Wellcome Trust's climate-sensitive infectious disease [modeling initiative](https://wellcome.org/news/digital-tools-climate-sensitive-infectious-disease). We aim to integrate open data sets of climatic and vegetation data with internationally-reported outbreak data to build an modeling pipeline that can be adapted to varying local conditions in RVF-prone regions across the continent.

### Pipeline Structure
The project pipeline is organized into two distinct modules: 1) the Data Acquisition Module and 2) the Modeling Framework Module. Both modules are orchestrated using the `targets` package in R, a powerful tool for creating reproducible and efficient data analysis workflows. By defining a workflow of interdependent tasks, known as 'targets', this package ensures that each step in the workflow is only executed when its inputs or code change, thereby optimizing computational efficiency. A modular, scalable, and transparent design makes `targets` an ideal choice for managing pipelines in reproducible research and production environments. An introduction to workflow management using `targets` can be found [here](https://books.ropensci.org/targets/). This project also uses the [{renv}](https://rstudio.github.io/renv/) framework to track R package dependencies and versions which are recorded in the `renv.lock` file. Code used to manage dependencies is in `renv/` and other files in the root project directory.  On starting an R session in the working directory, run ``renv::hydrate()` and `renv::restore()` to install required R packags and dependencies. 

### Repository Structure
Project code is available on the [open-rvfcast](https://github.com/ecohealthalliance/open-rvfcast) GitHub repository

-   `data/` contains downloaded and transformed data sources. These data are .gitignored and are available with access to the EHA open-rvf S3 bucket or the raw data can be download and processed.
-   `R/` contains functions used in this analysis. 
-   `reports/` contains literate code for  R Markdown reports generated in the analysis
-   `outputs/` contains compiled reports and figures.
  
### Data Storage

We utilized parquet files and the `arrow` package in R as our primary method of storing data. Parquet files are optimized for high-performance, out-of-memory data processing, making it well-suited for efficiently handling and processing large, complex datasets. Additionally, `arrow::open_dataset()` supports seamless integration with cloud storage, enabling direct access to remote datasets, which improves workflow efficiency and scalability when working with large, distributed data sources. While the data acquisition module requires the processing of large datasets, the final cleaned data can be accessed directly from the cloud simply via:

```
dataset <- open_dataset("s3://open-rvfcast/data/explanatory_variables")
```

Because parquet files are a columnar format with structured metadata available in each file, some operations, such as filtering, can be applied directly to remote datasets without having to first download the full data. The following will only download the model data for a single day:

```
dataset <- open_dataset("s3://open-rvfcast/data/explanatory_variables") |> filter(date == "2023-12-14") |> collect()
```

Due to the large nature of the data not every day is available - the dataset has been subsetted to two randomly chosen days per month between 2007 and 2024.

## 1. Data Acquisition Module

### Cloud Storage

Many of the computational steps in the first module can be time consuming and either depend on or produce large files. In order to speed up the pipeline, intermediate files can be stored on the cloud for rapid retrieval and portability between pipeline instances. We currently use an AWS [S3 bucket](https://aws.amazon.com/s3/) for this purpose. The pipeline will still run without access to cloud storage but the user can benefit from adapt the `_targets.R` file to use their own object storage repository. AWS access keys and bucket ID are stored in the `.env` file. 

### Data Access

Gaining access to the source data stores involves obtaining authentication credentials, such as API keys, tokens, and certificates, to ensure secure communication and data transfer. There are three primary sources of data that require access credentials
1. [ECMWF](https://www.ecmwf.int/): for accessing monthly weather forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF).
2. [COPERNICUS](https://dataspace.copernicus.eu/): for accessing Normalized Difference Vegetation Index (NDVI) data derived from the European Space Agency's  Sentinel-3 satellite.
3. [APPEEARS](https://appeears.earthdatacloud.nasa.gov/api/): for accessing historical NDVI data prior to the Sentinel-3 mission from NASA MODIS satellites.

### Data Sources

#### Static Data

The following data sources are static, or time-invariant. Raw static data was downloaded from the linked sources and joined with dynamic data, such as temperature, which varied by day. 

1. [Soil types]():
2. [Aspect]():
3. [Slope]():
4. [Gridded Livestock of the World (GLW)](): 
5. [Elevation]():
6. [Bioclimatic data]():
7. [Landcover type]():

#### Dynamic Data

Dynamic data sources are those that vary with time. The following sources make up the dynamic layers

1. [weather_anomalies]()
2. [forecasts_anomalies]()
3. [ndvi_anomalies]()
4. [wahis_outbreak_history]()

#### Temporal Covaraince

In order to isolate the influence of each dynamic predictor, which can be highly conflated with each other due to a shared dependence on time and long-term trends, we used the difference between current values and historical means instead of raw values for dynamic layers. This approach helped mitigate the strong correlation with time that naturally exists in environmental variables like temperature and NDVI. Seasonality was then accounted for by including year and day-of-year (DOY) as predictors in the model. 

#### Forecast Dynamic Data


#### Lagged Dynamic Data




#### Historical Outbreak Data




### Targets Pipeline

A visualization of the data acquisition module can be found below. Additional targets not shown are responsible for fetching and storing intermediate datasets on the cloud. To run the data acquisition module, download the repository from github and run the following command. Note, without access to the common S3 bucket store this pipeline will take a significant amount of time and space to run. In addition, without access to the remote data store, the data acquisition module must be run before running the modeling module.

```
tar_make(store = "data_aquisition_targets.R")
```

The schematic figure below summarizes the steps of the data acquisition module. The figure is generated using `mermaid.js` syntax and should display as a graph on GitHub.  It can also be viewed by pasting the code into <https://mermaid.live>.)

```{r, echo=FALSE, message = FALSE, results='asis'}
mer <- targets::tar_mermaid(targets_only = TRUE, outdated = FALSE, 
                            legend = FALSE, color = FALSE, 
                            exclude = c("readme", ends_with("_targets")))
cat(
  "```mermaid",
  mer[1], 
  #'Objects([""Objects""]) --- Functions>""Functions""]',
  'subgraph Project Workflow',
  mer[3:length(mer)],
  'linkStyle 0 stroke-width:0px;',
  "```",
  sep = "\n"
)
```


## 2. Rift Valley Fever (RVF) risk model pipeline


Follow the links for more information about:

- [`targets`](https://ecohealthalliance.github.io/eha-ma-handbook/3-projects.html#targets)
- [`renv`](https://ecohealthalliance.github.io/eha-ma-handbook/3-projects.html#package-management-with-renv)  
- [git-crypt](https://ecohealthalliance.github.io/eha-ma-handbook/16-encryption.html)
- [Reproducible workflows](https://github.com/ecohealthalliance/building-blocks-of-reproducibility)

