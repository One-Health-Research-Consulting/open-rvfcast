<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>readme</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="README_files/libs/clipboard/clipboard.min.js"></script>
<script src="README_files/libs/quarto-html/quarto.js"></script>
<script src="README_files/libs/quarto-html/popper.min.js"></script>
<script src="README_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="README_files/libs/quarto-html/anchor.min.js"></script>
<link href="README_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="README_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="README_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="README_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="README_files/libs/bootstrap/bootstrap-973236bd072d72a04ee9cd82dcc9cb29.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<!-- README.md is generated from README.Rmd. Please edit that file -->
<section id="an-open-source-framework-for-rift-valley-fever-forecasting" class="level1">
<h1>An open-source framework for Rift Valley Fever forecasting</h1>
<!-- badges: start -->
<p><a href="https://www.repostatus.org/#wip"><img src="https://www.repostatus.org/badges/latest/wip.svg" class="img-fluid" alt="Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public."></a> <a href="https://lifecycle.r-lib.org/articles/stages.html#experimental"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" class="img-fluid" alt="Lifecycle: experimental"></a> <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License%20(for%20code)-MIT-green.svg" class="img-fluid" alt="License (for code): MIT"></a> <a href="http://creativecommons.org/publicdomain/zero/1.0/"><img src="https://img.shields.io/badge/License%20(for%20data)-CC0_1.0-lightgrey.svg" class="img-fluid" alt="License: CC0-1.0"></a> <a href="http://creativecommons.org/publicdomain/zero/1.0/"><img src="https://img.shields.io/badge/License%20(for%20text)-CC_BY_4.0-blue.svg" class="img-fluid" alt="License: CC-BY-4.0"></a> <!-- badges: end --></p>
</section>
<section id="overview-of-openrvfcast" class="level1">
<h1>Overview of OpenRVFcast</h1>
<p>The goal of EcoHealth Alliance’s ongoing OpenRVFcast project the development of a generalizable, open-source modeling framework for predicting Rift Valley Fever (RVF) outbreaks in Africa, funded by the Wellcome Trust’s climate-sensitive infectious disease <a href="https://wellcome.org/news/digital-tools-climate-sensitive-infectious-disease">modeling initiative</a>. We aim to integrate open data sets of climatic and vegetation data with internationally-reported outbreak data to build an modeling pipeline that can be adapted to varying local conditions in RVF-prone regions across the continent.</p>
<p>This project is a collaborative effort between <a href="https://www.ecohealthalliance.org/">EcoHealth Alliance</a>, [INSERT PARTNER LINKS]</p>
<section id="pipeline-structure" class="level3">
<h3 class="anchored" data-anchor-id="pipeline-structure">Pipeline Structure</h3>
<p>The project pipeline is organized into two distinct modules: 1) the <strong>Data Acquisition Module</strong> and 2) the <strong>Modeling Framework Module</strong>. Both modules are orchestrated using the <code>targets</code> package in R, a powerful tool for creating reproducible and efficient data analysis workflows. By defining a workflow of interdependent tasks, known as ‘targets’, this package ensures that each step in the workflow is only executed when its inputs or code change, thereby optimizing computational efficiency. A modular, scalable, and transparent design makes <code>targets</code> an ideal choice for managing pipelines in reproducible research and production environments. An introduction to workflow management using <code>targets</code> can be found <a href="https://books.ropensci.org/targets/">here</a>. This project also uses the <a href="https://rstudio.github.io/renv/">{renv}</a> framework to track R package dependencies and versions which are recorded in the <code>renv.lock</code> file. Code used to manage dependencies is in <code>renv/</code> and other files in the root project directory. On starting an R session in the working directory, run `<code>renv::hydrate()</code> and <code>renv::restore()</code> to install required R packags and dependencies.</p>
</section>
<section id="repository-structure" class="level3">
<h3 class="anchored" data-anchor-id="repository-structure">Repository Structure</h3>
<p>Project code is available on the <a href="https://github.com/ecohealthalliance/open-rvfcast">open-rvfcast</a> GitHub repository which is organized with the following structure:</p>
<ul>
<li><code>data/</code> contains downloaded and transformed data sources. These data are .gitignored and are available with access to the EHA open-rvf S3 bucket or the raw data can be download and processed.</li>
<li><code>R/</code> contains functions used in this analysis.</li>
<li><code>reports/</code> contains literate code for R Markdown reports generated in the analysis.</li>
<li><code>outputs/</code> contains compiled reports and figures.</li>
</ul>
</section>
<section id="data-storage" class="level3">
<h3 class="anchored" data-anchor-id="data-storage">Data Storage</h3>
<p>We utilized parquet files and the <code>arrow</code> package in R as our primary method of storing data. Parquet files are optimized for high-performance, out-of-memory data processing, making it well-suited for efficiently handling and processing large, complex datasets. Additionally, <code>arrow::open_dataset()</code> supports seamless integration with cloud storage, enabling direct access to remote datasets, which improves workflow efficiency and scalability when working with large, distributed data sources. While the data acquisition module requires the processing of large datasets, the final cleaned data can be accessed directly from the cloud by opening the following connection:</p>
<pre><code>dataset &lt;- arrow::open_dataset(arrow::s3_bucket("open-rvfcast/data/africa_full_data/", anonymous = TRUE))
dataset$schema</code></pre>
<p>As parquet files are a columnar format with structured metadata available in each file, some operations, such as filtering, summarizing, and inspecting the data schema can be applied directly to remote datasets without having to first download the full data. Calling collect() on the dataset will initiate the download. For example, the following will filter the data and then download the model data for a single day:</p>
<pre><code>dataset &lt;- arrow::open_dataset(arrow::s3_bucket("open-rvfcast/data/africa_full_data/", anonymous = TRUE)) |&gt;
  dplyr::filter(date == "2005-01-08") |&gt; 
  dplyr::collect()

dataset</code></pre>
<p>However, due to computational demands of such large data, the model analysis pipeline will download the data in entirety before analysis. In addition, the dataset has been subsetted to two randomly chosen days per month between 2007 and 2024. Future work will provide the entire dataset for every day between 2005 and the current year.</p>
<p>The data targets that are subsetted this way are:</p>
<ol type="1">
<li>weather_anomalies</li>
<li>weather_anomalies_lagged</li>
<li>forecasts_anomalies</li>
<li>forecast_anomalies_lagged</li>
<li>ndvi_anomalies</li>
<li>ndvi_anomalies_lagged</li>
<li>rvf_response</li>
<li>africa_full_model_data</li>
</ol>
</section>
<section id="data-acquisition-module" class="level2">
<h2 class="anchored" data-anchor-id="data-acquisition-module">1. Data Acquisition Module</h2>
<section id="cloud-storage" class="level3">
<h3 class="anchored" data-anchor-id="cloud-storage">Cloud Storage</h3>
<p>Many of the computational steps in the first module can be time consuming and either depend on or produce large files. In order to speed up the pipeline, intermediate files can be stored on the cloud for portability. We currently use an AWS <a href="https://aws.amazon.com/s3/">S3 bucket</a> for this purpose. The pipeline will still run without access to cloud storage, but users can add their own AWS access keys and bucket ID to the <code>.env</code> file to enable cloud storage.</p>
<p>Environment variables to add to the .env file:</p>
<pre><code>AWS_DEFAULT_REGION=
AWS_REGION=
AWS_BUCKET_ID=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=</code></pre>
</section>
<section id="data-access" class="level3">
<h3 class="anchored" data-anchor-id="data-access">Data Access</h3>
<p>Acquiring the raw source data stores involves first obtaining authentication credentials, such as API keys, tokens, and certificates. There are three primary sources of data that require access credentials 1. <a href="https://www.ecmwf.int/">ECMWF</a>: for accessing monthly weather forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF). 2. <a href="https://dataspace.copernicus.eu/">COPERNICUS</a>: for accessing Normalized Difference Vegetation Index (NDVI) data derived from the European Space Agency’s Sentinel-3 satellite. 3. <a href="https://appeears.earthdatacloud.nasa.gov/api/">APPEEARS</a>: for accessing historical NDVI data prior to the Sentinel-3 mission from NASA MODIS satellites.</p>
<p>Before running the data acquisition pipeline, credentials for all three sources must be added to the .env file</p>
<p>Environment variables to add to the .env file:</p>
<pre><code>ECMWF_USERID=
ECMWF_TOKEN=
COPERNICUS_USERNAME=
COPERNICUS_PASSWORD=
APPEEARS_USERNAME=
APPEEARS_PASSWORD=
APPEEARS_TOKEN=</code></pre>
</section>
<section id="data-sources" class="level3">
<h3 class="anchored" data-anchor-id="data-sources">Data Sources</h3>
<p>All spatial data were interpolated to a resolution of 0.1° across Africa and standardized to the WGS 84 coordinate reference system. All temporal data layers were joined by date.</p>
<p>If data files become corrupted they can be re-generated from the raw sources by setting the <code>OVERWRITE_X</code> flags to TRUE in the .env file. This will prevent the pipeline from first downloading the data on AWS, re-download and process the raw data from the original sources, and upload the processed files to AWS for future use. Note that, under normal use, these should always be set to FALSE. The pipeline will automatically download any missing data without having to change these settings. This is only to replace data that has already been downloaded and processed mainly for pipeline development purposes.</p>
<section id="the-response-variable" class="level4">
<h4 class="anchored" data-anchor-id="the-response-variable">The Response Variable</h4>
<p>The goal of this project is to evaluate the potential for an outbreak of Rift Valley fever (RVF) to occur across Africa. The model was trained against a binary variable representing whether or not an outbreak occurred at each spatial location 0-30 days, 30-60 days, 60-90 days, 90-120 days, and 120-150 days after every date. RVF outbreak data was provided by the <a href="https://www.woah.org/en/home/">World Animal Health Information System (WOAH)</a> and accessed via a <a href="https://www.dolthub.com/csv/ecohealthalliance/wahisdb/main/wahis_outbreaks">database</a> of cleaned outbreak data managed by EcoHealth Alliance.</p>
<ol type="1">
<li>RVF_occurance: A binary factor reflecting RVF occurance at each location across the 5 forecast intervals.</li>
</ol>
</section>
<section id="static-data" class="level4">
<h4 class="anchored" data-anchor-id="static-data">Static Data</h4>
<p>The following data sources are static, or time-invariant. Raw static data was downloaded from the linked sources and joined with dynamic data, such as temperature, which varied by day.</p>
<ol start="2" type="1">
<li><a href="https://www.fao.org/soils-portal/data-hub/soil-maps-and-databases/harmonized-world-soil-database-v20/en/">Soil types</a>: Soil types based on the Food and Agriculture Organization of the United Nations (<a href="https://www.fao.org/home/en">FAO</a>) Harmonized World Soil Database v2.0 (HWSD) with soil types aggregated into 8 categories: clay (heavy) + clay loam (1), silt loam + silty clay (2), sandy clay + clay (3), loam + silty clay loam (4), sandy clay loam (5), sandy loam + silt (6), loamy sand + silt loam (7), and sand (8) based on similarity in the USDA sand-silt-clay ternary texture class diagram (<a href="https://www.fao.org/soils-portal/data-hub/soil-maps-and-databases/harmonized-world-soil-database-v20/en/">Figure 2</a>). Data was aggregated by identifying the most common slope or aspect within each 0.1 degree grid cell.</li>
<li><a href="Global%20Terrain%20Slope%20and%20Aspect%20Data">Slope and Aspect</a>: Slope and aspect data from the FAO Global Terrain Slope and Aspect</li>
<li><a href="https://www.nature.com/articles/sdata2018227">Gridded Livestock of the World 3 (GLW3)</a>: Global distribution data included <a href="https://dataverse.harvard.edu/api/access/datafile/6769710">cattle</a>, <a href="https://dataverse.harvard.edu/api/access/datafile/6769629">sheep</a>, and <a href="https://dataverse.harvard.edu/api/access/datafile/6769692">goats</a> censused in 2010 and available at a native resolution of 5 arc-minutes. Data was accessed via the <a href="https://dataverse.harvard.edu/">Harvard dataverse</a>.</li>
<li><a href="https://srtm.csi.cgiar.org/">Elevation</a>: Elevation data accessed via the <code>elevation_global()</code> function of the <a href="https://rdrr.io/cran/geodata/man/elevation.html">geodata</a> package in R, drawn from the Shuttle Radar Topography Mission (SRTM) at resolution of 0.5 minutes of a degree.</li>
<li><a href="https://www.worldclim.org/data/bioclim.html">Bioclimatic data*</a>: Bioclimactic data from the WorldClim version 2.1 accessed via the <code>worldclim_global()</code> function of the <a href="https://rdrr.io/cran/geodata/man/worldclim.html">geodata</a> package in R and represent the global mean values across the period of 1970-2000 at a 2.5m resolution.</li>
<li><a href="https://search.r-project.org/CRAN/refmans/geodata/html/landcover.html">Landcover type</a>: Landcover data was accessed via the <code>landcover()</code> function of <a href="https://rdrr.io/cran/geodata/man/elevation.html">geodata</a> package in R, drawn from the ESA WorldCover Database with a spatial resolution of 30 arc-seconds. Values for each landcover type (trees, grassland, shrubs, cropland, built, bare, snow, water, wetland, mangroves, and moss), reflect the fraction of each a landcover class at each location.</li>
</ol>
<p><small>* Bioclimactic variables included: Annual_Mean_Temperature, Mean_Diurnal_Range, Isothermality, Temperature_Seasonality, Max_Temperature_of_Warmest_Month, Min_Temperature_of_Coldest_Month, Temperature_Annual_Range, Mean_Temperature_of_Wettest_Quarter, Mean_Temperature_of_Driest_Quarter, Mean_Temperature_of_Warmest_Quarter, Mean_Temperature_of_Coldest_Quarter, Annual_Precipitation, Precipitation_of_Wettest_Month, Precipitation_of_Driest_Month, Precipitation_Seasonality, Precipitation_of_Wettest_Quarter, Precipitation_of_Driest_Quarter, Precipitation_of_Warmest_Quarter, and Precipitation_of_Coldest_Quarter</small></p>
</section>
<section id="dynamic-data" class="level4">
<h4 class="anchored" data-anchor-id="dynamic-data">Dynamic Data</h4>
<p>Dynamic data sources are those that vary with time. Dynamic predictors can be highly conflated with each other due to a shared dependence on time, to account for this shared dependence, we used calculated the anomaly, or difference between current values and historical means, instead of using the raw values. Anomalies were calculated by first determining the difference between the current value and its historical mean for that day-of-year (DOY) and scaled by dividing by the standard deviation for that DOY. Focusing on anomalous values helped mitigate the strong correlation with time that naturally exists in environmental variables like temperature and NDVI. Seasonality was then accounted for by including year and day-of-year (DOY) as predictors in the model. The following sources make up the dynamic layers:</p>
<ol start="8" type="1">
<li><a href="">weather_anomalies</a>: NASA weather data was acquired across Africa using the <code>get_power()</code> function of the <a href="https://docs.ropensci.org/nasapower/">nasapower</a> package in R which provides access to NASA meteorological data from the <a href="https://power.larc.nasa.gov/">NASAPOWER</a> project. The difference, or anomaly value, was then found by subtracting each weather value from the average value for that day-of-year (DOY).</li>
<li>ndvi_anomalies: NDVI data was sourced from both the NASA’s Moderate Resolution Imaging Spectroradiometer (<a href="https://modis.gsfc.nasa.gov/data/dataprod/mod13.php">MODIS</a>) and the European Space Agency’s Copernicus <a href="https://user.eumetsat.int/catalogue/EO:EUM:DAT:0340">Sentinel-3</a> missions. MODIS is due to be retired in 2025 while Sentinel-3 NDVI data is available from September 2018. MODIS and Sentinel-3 NDVI values were interpolated to a daily interval from their native 16 day (MODIS) and ~10 day (Sentinel-3) intervals using a step-function. NDVI values were averaged when data from both sources were available. The difference, or anomaly value, was then found by subtracting NDVI from the average value for that day-of-year (DOY).</li>
</ol>
<section id="weather-forecasts" class="level5">
<h5 class="anchored" data-anchor-id="weather-forecasts">Weather Forecasts</h5>
<ol start="10" type="1">
<li><a href="https://cds.climate.copernicus.eu/datasets/seasonal-monthly-single-levels?tab=overview">ecmwf_forecasts</a> We also included long-range projections of future weather provided by the European Centre for Medium-Range Weather Forecasts (ECMWF) and accessed through the <a href="https://cds.climate.copernicus.eu/">Copernicus Climate Data Store (CDS)</a>. The projected data represent the mean of a 51-member ensemble and include the expected average temperature, precipitation, and relative humidity for each location across different forecast intervals. Historical forecasts were available through hindcasts, which apply the current forecasting methods to historical data to simulate what forecasts would have been available at those times based on past conditions.</li>
</ol>
</section>
<section id="lagged-dynamic-data" class="level5">
<h5 class="anchored" data-anchor-id="lagged-dynamic-data">Lagged Dynamic Data</h5>
<p>Outbreak occurrence is not always directly influenced by the immediately preceding conditions. Biological systems often involve delayed responses. For example, heavy precipitation may promote a mosquito hatch, which can lead to an outbreak only after a delay. To account for the influence of past environmental conditions, we included lagged weather and NDVI data, specifically the average values from 0-30, 30-60, 60-90, 90-120, and 120-150 days prior.</p>
<ol start="11" type="1">
<li>weather_anomalies: Average weather anomaly values lagged over the previous 1-5 months</li>
<li>ndvi_anomalies_lagged: Average NDVI anomaly values lagged over the previous 1-5 months</li>
</ol>
</section>
<section id="historical-outbreak-data" class="level5">
<h5 class="anchored" data-anchor-id="historical-outbreak-data">Historical Outbreak Data</h5>
<p>An important factor in evaluating the potential for a future outbreak is the history of outbreaks in a region. Recent nearby outbreaks can amplify the likelihood of an outbreak occurring at a given location, while older outbreaks might reduce the risk by influencing the resistance landscape, reflecting a history of prior exposure to the disease.</p>
<p>To account for the influence of outbreak history, we generated outbreak exposure weights for both recent and historical outbreaks. These weights were determined using a function that decreases with distance from the source, modeling exposure as declining exponentially outward to a maximum distance of 500 km with an exponential rate of decay of 0.01km<sup>-1</sup>. Similarly, the effects of an outbreak were assumed to fade over time, with influence declining as time elapsed since the outbreak increased out to a maximum of 10 years at an exponential rate of decay of 0.5year<sup>-1</sup>. Outbreaks that occurred within the last 3 months were classified as ‘recent’ and included as a separate predictor in the model allowing them to have a different effect on the model outcome compared to the older outbreak exposures.</p>
<ol start="13" type="1">
<li>outbreak_history: Outbreak history was calculated using the data provided from same data described in the response section (item 1) above. As outbreak history contains information about the state of variable being predicted, special care was taken when splitting the data into test and training datasets to prevent data leakage described further below.</li>
</ol>
</section>
</section>
</section>
<section id="targets-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="targets-pipeline">Targets Pipeline</h3>
<p>A visualization of the data acquisition module can be found below. Additional targets not shown are responsible for fetching and storing intermediate datasets on the cloud. To run the data acquisition module, download the repository from github and run the following command. Note, without access to the common S3 bucket store this pipeline will take a significant amount of time and space to run. In addition, without access to the remote data store, the data acquisition module must be run before running the modeling module.</p>
<p>The output of the data acquisition pipeline is an Africa wide dataset produced by joining every source above together. There are two versions. <code>africa_full_data</code> and <code>africa_full_rvf_model_data</code>. The first is the full dataset of explanatory variables and the second pairs that with RVF outbreak data as the response variable. The following command will start the data acquisition pipeline, either downloading pre-processed files from an AWS bucket or other cloud based resource or re-generating the data from the raw sources. The expected size of the full pipeline is &gt;200Gb and so will require significant storage and time to run. Each day in the africa_ful_rvf_model_data is saved as a separate parquet file of ~500Mb.</p>
<pre><code>tar_make(africa_full_rvf_model_data, script = "data_acquisition_targets.R")</code></pre>
<p>The schematic figure below summarizes the steps of the data acquisition module. The figure is generated using <code>mermaid.js</code> syntax and should display as a graph on GitHub. It can also be viewed by pasting the code into <a href="https://mermaid.live" class="uri">https://mermaid.live</a>.)</p>
<!-- # ```{r, echo=FALSE, message = FALSE, results='asis'} -->
<!-- # mer <- targets::tar_mermaid(targets_only = TRUE,  -->
<!-- #                             outdated = FALSE,  -->
<!-- #                             legend = FALSE,  -->
<!-- #                             color = FALSE,  -->
<!-- #                             script = "data_acquisition_targets.R", -->
<!-- #                             exclude = c("readme", contains("AWS"))) -->
<!-- # cat( -->
<!-- #   "```mermaid", -->
<!-- #   mer[1],  -->
<!-- #   #'Objects([""Objects""]) --- Functions>""Functions""]', -->
<!-- #   'subgraph Project Workflow', -->
<!-- #   mer[3:length(mer)], -->
<!-- #   'linkStyle 0 stroke-width:0px;', -->
<!-- #   "```", -->
<!-- #   sep = "\n" -->
<!-- # ) -->
<!-- # ``` -->
</section>
</section>
<section id="rift-valley-fever-rvf-risk-model-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="rift-valley-fever-rvf-risk-model-pipeline">2. Rift Valley Fever (RVF) risk model pipeline</h2>
<section id="data-partitioning" class="level3">
<h3 class="anchored" data-anchor-id="data-partitioning">Data Partitioning</h3>
<p>Splitting data into training, validation, and test sets is an important step for building robust and reliable models. The training set is used to learn model parameters, the validation set helps fine-tune hyperparameters and prevent overfitting, and the test set provides an unbiased evaluation of the model’s performance on unseen data. Proper splitting ensures the model generalizes well to new data, avoiding issues like data leakage or over-optimistic performance estimates.</p>
<p>However, splitting outbreak data can be particularly challenging due to spatial and temporal clustering, which can lead to imbalanced or non-representative splits. Ensuring that all three splits contain representative data, including both outbreak presence and absence, is critical for robust model evaluation.</p>
<section id="spatial-splitting" class="level4">
<h4 class="anchored" data-anchor-id="spatial-splitting">Spatial splitting</h4>
<p>Spatial splitting was accomplished by <a href="https://nsojournals.onlinelibrary.wiley.com/doi/10.1111/ecog.02881">spatial blocking</a> using the spatial_block_cv() function of the <a href="https://spatialsample.tidymodels.org/">spatialsample</a> to create spatial cross-validation folds. This ensures that each split contains distinct spatial regions, at the level of municipality that contain representive information in all three splits.</p>
</section>
<section id="temporal-splitting" class="level4">
<h4 class="anchored" data-anchor-id="temporal-splitting">Temporal splitting</h4>
<p>In addition to spatial clustering, outbreak data is time-series by nature, necessitating techniques like expanding window splitting where the training set grows incrementally over time as more data becomes available. This approach is particularly suited for scenarios where temporal dependencies exist, and models must be evaluated on their ability to generalize to future, unseen data. When outbreaks are rare, subdividing the limited positive detections can exacerbate the imbalance, making it harder to accurately assess the model’s performance and generalizability.</p>
</section>
</section>
<section id="model-structure" class="level3">
<h3 class="anchored" data-anchor-id="model-structure">Model Structure</h3>
</section>
<section id="evaluating-model-performance" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-model-performance">Evaluating Model Performance</h3>
</section>
<section id="generating-dynamic-documentation-and-reports" class="level3">
<h3 class="anchored" data-anchor-id="generating-dynamic-documentation-and-reports">Generating Dynamic Documentation and Reports</h3>
</section>
<section id="targets-pipeline-1" class="level3">
<h3 class="anchored" data-anchor-id="targets-pipeline-1">Targets Pipeline</h3>
<p>A visualization of the data acquisition module can be found below.</p>
<pre><code>targets::tar_make(script = "model_framework_targets.R")</code></pre>
<p>The schematic figure below summarizes the steps of the data acquisition module. The figure is generated using <code>mermaid.js</code> syntax and should display as a graph on GitHub. It can also be viewed by pasting the code into <a href="https://mermaid.live" class="uri">https://mermaid.live</a>.)</p>
<!-- # ```{r, echo=FALSE, message = FALSE, results='asis'} -->
<!-- # mer <- targets::tar_mermaid(targets_only = TRUE,  -->
<!-- #                             outdated = FALSE,  -->
<!-- #                             legend = FALSE,  -->
<!-- #                             color = FALSE,  -->
<!-- #                             script = "model_framework_targets.R", -->
<!-- #                             exclude = c("readme", contains("AWS"))) -->
<!-- # cat( -->
<!-- #   "```mermaid", -->
<!-- #   mer[1],  -->
<!-- #   #'Objects([""Objects""]) --- Functions>""Functions""]', -->
<!-- #   'subgraph Project Workflow', -->
<!-- #   mer[3:length(mer)], -->
<!-- #   'linkStyle 0 stroke-width:0px;', -->
<!-- #   "```", -->
<!-- #   sep = "\n" -->
<!-- # ) -->
<!-- # ``` -->
<p><a href="https://github.com/ropensci/waywiser"><code>Waywiser</code></a></p>
<p>Follow the links for more information about:</p>
<ul>
<li><a href="https://ecohealthalliance.github.io/eha-ma-handbook/3-projects.html#targets"><code>targets</code></a></li>
<li><a href="https://ecohealthalliance.github.io/eha-ma-handbook/3-projects.html#package-management-with-renv"><code>renv</code></a><br>
</li>
<li><a href="https://ecohealthalliance.github.io/eha-ma-handbook/16-encryption.html"><code>git-crypt</code></a></li>
<li><a href="https://github.com/ecohealthalliance/building-blocks-of-reproducibility"><code>Reproducible workflows</code></a></li>
</ul>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>